{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773d2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fa14f1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=spark://spark-master:7077) created by __init__ at <ipython-input-2-9c7ba4625174>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-9c7ba4625174>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mconf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSparkConf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetMaster\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'spark://spark-master:7077'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mspark_context\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    131\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[1;32m    132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 133\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    134\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    339\u001B[0m                         \u001B[0;34m\" created by %s at %s:%s \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    340\u001B[0m                         % (currentAppName, currentMaster,\n\u001B[0;32m--> 341\u001B[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001B[0m\u001B[1;32m    342\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    343\u001B[0m                     \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minstance\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=spark://spark-master:7077) created by __init__ at <ipython-input-2-9c7ba4625174>:3 "
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster('spark://spark-master:7077')\n",
    "spark_context = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c7bbe233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открытие текстового файла и превращение его в RDD.\n",
    "# К докеру со spark-worker примаунчена папка /user-data с вашего диска, в которой лежат скачанные данные.\n",
    "data = spark_context.textFile('/user-data/combined/reviews.csv')\n",
    "# Берём первую строку, для дальнейшей фильтрации. Первая строка отвечает за заголовки, они нас не интересуют.\n",
    "header_data = data.first()\n",
    "# Начались вычисления. Первая функция пропускает все данные кроме первой строки с заголовками.\n",
    "# Функция `map` разделяет строку на колонки, используя разделитель `,`.\n",
    "result_data = data.filter(lambda row: row != header_data) \\\n",
    "    .map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dd7e9de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bj',\n",
       "  '0_bj',\n",
       "  'Ilovebennjerry',\n",
       "  '2017-04-15',\n",
       "  '3',\n",
       "  'Not enough brownies!',\n",
       "  '10.0',\n",
       "  '3.0',\n",
       "  '\"Super good',\n",
       "  \" don't get me wrong. But I came for the caramel and brownies\",\n",
       "  \" not the sweet cream. The packaging made it seem like brownies were packed and bountiful *crying frowny emoji* I'd say the taste of this was amazing\",\n",
       "  \" but the ratio of brownie to sweet cream was disappointing. Liked it regardless but probably won't buy again simply because it didn't live up to its promising package. I'll find another one that has a better ratio and wayyy more yummy chewy brownies.\"],\n",
       " ['Overall',\n",
       "  ' good flavor',\n",
       "  ' texture',\n",
       "  ' idea',\n",
       "  ' and brownies. Not so great caramel/sweet cream/ brownie RATIO. Just add more brownies. Please.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Sweettooth909',\n",
       "  '2020-01-05',\n",
       "  '5',\n",
       "  'I’m OBSESSED with this pint!',\n",
       "  '3.0',\n",
       "  '0.0',\n",
       "  '\"I decided to try it out although I’m not a huge caramel fan',\n",
       "  ' and the first buy was ok',\n",
       "  ' didn’t like the caramel too much and for some reason that specific pint barely had any brownies! Like there were some on top but from the middle to the bottom? Zilch. Nada. Nothing! It was disappointing. But for some reason',\n",
       "  ' I bought it again and I really do believe it was just that point bcuz the second one was glorious!! I had a big brownie chunk in EVERY bite. Plus the caramel didn’t bother me as much and I took a liking to it. Conclusion: I’m on my fifth pint',\n",
       "  ' of the week and I’m going back to get my daily pint cus I’m kinda sick and need some happiness.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'LaTanga71',\n",
       "  '2018-04-26',\n",
       "  '3',\n",
       "  'My favorite...More Caramel Please',\n",
       "  '5.0',\n",
       "  '2.0',\n",
       "  '\"My caramel core begins to disappear about half way through. I only buy this for the caramel core. The first time I purchased this I was not even aware that there were blonde bits in there. You had me a salted caramel core',\n",
       "  \" well this is the fourth pint that I have purchased and the caramel is plentiful at the top then it begins to get sparse about half way down at 3/4s of the way down its gone. It works out in my husbands favor because he doesn't care for the caramel so he gets to finish my pint while having a full pint of his choice. Please please please\",\n",
       "  ' its called salted caramel core for a reason. Let the caramel flow through the core. P.S. You could put a little extra in there',\n",
       "  ' I totally would not mind.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'chicago220',\n",
       "  '2018-01-14',\n",
       "  '5',\n",
       "  'Obsessed!!!',\n",
       "  '24.0',\n",
       "  '1.0',\n",
       "  '\"Why are people complaining about the blonde brownies? If you don’t like blonde brownies',\n",
       "  ' get ice cream with fudge brownies! I LOVE the blonde brownies and cannot get enough of them. This ice cream is by far my favorite. I have driven to multiple stores to try and find it... and when I do',\n",
       "  ' I stock up! I’ve tried other flavors when I can’t find this one',\n",
       "  ' and none of them even come close to making me as happy as this does! I’d say sometimes the caramel to brownie ratio is a little off at times (more brownies please!)',\n",
       "  ' but I’ll still always enjoy it nevertheless!\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Kassidyk',\n",
       "  '2020-07-24',\n",
       "  '1',\n",
       "  'Worst Ice Cream Ever!',\n",
       "  '1.0',\n",
       "  '5.0',\n",
       "  '\"This ice cream is worst ice cream I’ve ever tasted. I was beyond excited when I picked it up today and couldn’t wait to get home and try it. First the sweet cream ice cream has no flavour. But that’s fine I could get past that',\n",
       "  ' but the salted caramel core is what’s really wrong. It’s disgusting and so salty it’s actually inedible. I’ve had ice cream before that I have really enjoyed but never to the point where I physically can not stomach it. I love salted caramel but this is just another level. I actually came here to read the reviews and see how others thought of it just in case mine was a bad batch. Never again!\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Nikiera',\n",
       "  '2020-07-23',\n",
       "  '2',\n",
       "  'Way Too Salty',\n",
       "  '3.0',\n",
       "  '1.0',\n",
       "  '\"I bought this last night to go with Louisiana Crunch cake because I was craving that salty and sweet flavor. Unfortunately',\n",
       "  ' I was not thrilled with the salted caramel core of this ice cream. Usually',\n",
       "  ' when you have salted caramel',\n",
       "  ' the salt enhances the flavor of the caramel. The salt in this core OVERWHELMED everything',\n",
       "  \" even the cake. It was disappointing to not enjoy my dessert and I doubt I'll be finishing this pint of ice cream\",\n",
       "  ' which is a shame because you should never have to throw away ice cream.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Mmelvin',\n",
       "  '2017-05-28',\n",
       "  '3',\n",
       "  '\"Love this flavor',\n",
       "  ' but...\"',\n",
       "  '3.0',\n",
       "  '3.0',\n",
       "  '\"This is definitely my favorite flavor',\n",
       "  ' but recently',\n",
       "  \" it does not have enough blonde brownie chunks. I'll go through at least half of the pint with just 1 or 2 chunks. It looks like plain ice cream with a salted caramel core. This has happened a lot lately\",\n",
       "  \" and it's so dissapointing. I've gotten to the point where I simply stopped buying it. Today\",\n",
       "  ' purchased two because they were on sale. It started the way it should be',\n",
       "  ' with plenty of yummy chunks. Before I even got halfway as I was serving myself',\n",
       "  ' no more chunks. After some digging around',\n",
       "  ' I found 3 more way at the bottom. Please fix this! I love this ice cream',\n",
       "  ' but it\\'s pointless without the chunks.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Shay10',\n",
       "  '2017-07-02',\n",
       "  '3',\n",
       "  'Really Wanted To Love This',\n",
       "  '4.0',\n",
       "  '1.0',\n",
       "  '\"I am not a chocolate person at all',\n",
       "  \" and this was my very first time trying Ben & Jerry's\",\n",
       "  \" (Shame on me I'm 23 lol) so I was so excited to find a Caramel Ice Cream with BLONDE Brownie chunks instead of fudge. I really wanted to like this ice cream but I just couldn't. The Salted Caramel is a little too salty\",\n",
       "  ' In fact',\n",
       "  \" I couldn't even eat the core\",\n",
       "  \" I just ate around it. I didn't really like the Sweet Cream Ice Cream\",\n",
       "  \" it didn't really have a taste to me\",\n",
       "  ' (Maybe Vanilla would be better). The only thing I loved about this Ice Cream were the Blonde Brownie Chunks',\n",
       "  ' They were so delicious! I would love for this Ice Cream Recipe to be worked on',\n",
       "  ' maybe I\\'ll give it a try again in the future.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'caramel4dayz',\n",
       "  '2017-07-16',\n",
       "  '2',\n",
       "  'Could be better.',\n",
       "  '8.0',\n",
       "  '6.0',\n",
       "  '\"I LOVE caramel',\n",
       "  ' so much so that I could eat it off a spoon',\n",
       "  ' which was exactly what I was planning to do when I bought this ice cream. Unfortunately this caramel core is way too salty. I actually eat around it! I\\'m not a fan of the sweet cream ice cream flavor either; vanilla would\\'ve worked better. The blondie pieces were delicious but overall I\\'m disappointed. I really wanted to love this since I love Ben & Jerry\\'s but this was not up to par with their other flavors!\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '']]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ced4936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = result_data.filter(lambda line: len(line) > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5a4c885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = result_data.filter(lambda line: line[4].isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "13f621ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bj',\n",
       "  '0_bj',\n",
       "  'Ilovebennjerry',\n",
       "  '2017-04-15',\n",
       "  '3',\n",
       "  'Not enough brownies!',\n",
       "  '10.0',\n",
       "  '3.0',\n",
       "  '\"Super good',\n",
       "  \" don't get me wrong. But I came for the caramel and brownies\",\n",
       "  \" not the sweet cream. The packaging made it seem like brownies were packed and bountiful *crying frowny emoji* I'd say the taste of this was amazing\",\n",
       "  \" but the ratio of brownie to sweet cream was disappointing. Liked it regardless but probably won't buy again simply because it didn't live up to its promising package. I'll find another one that has a better ratio and wayyy more yummy chewy brownies.\"],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Sweettooth909',\n",
       "  '2020-01-05',\n",
       "  '5',\n",
       "  'I’m OBSESSED with this pint!',\n",
       "  '3.0',\n",
       "  '0.0',\n",
       "  '\"I decided to try it out although I’m not a huge caramel fan',\n",
       "  ' and the first buy was ok',\n",
       "  ' didn’t like the caramel too much and for some reason that specific pint barely had any brownies! Like there were some on top but from the middle to the bottom? Zilch. Nada. Nothing! It was disappointing. But for some reason',\n",
       "  ' I bought it again and I really do believe it was just that point bcuz the second one was glorious!! I had a big brownie chunk in EVERY bite. Plus the caramel didn’t bother me as much and I took a liking to it. Conclusion: I’m on my fifth pint',\n",
       "  ' of the week and I’m going back to get my daily pint cus I’m kinda sick and need some happiness.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'LaTanga71',\n",
       "  '2018-04-26',\n",
       "  '3',\n",
       "  'My favorite...More Caramel Please',\n",
       "  '5.0',\n",
       "  '2.0',\n",
       "  '\"My caramel core begins to disappear about half way through. I only buy this for the caramel core. The first time I purchased this I was not even aware that there were blonde bits in there. You had me a salted caramel core',\n",
       "  \" well this is the fourth pint that I have purchased and the caramel is plentiful at the top then it begins to get sparse about half way down at 3/4s of the way down its gone. It works out in my husbands favor because he doesn't care for the caramel so he gets to finish my pint while having a full pint of his choice. Please please please\",\n",
       "  ' its called salted caramel core for a reason. Let the caramel flow through the core. P.S. You could put a little extra in there',\n",
       "  ' I totally would not mind.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'chicago220',\n",
       "  '2018-01-14',\n",
       "  '5',\n",
       "  'Obsessed!!!',\n",
       "  '24.0',\n",
       "  '1.0',\n",
       "  '\"Why are people complaining about the blonde brownies? If you don’t like blonde brownies',\n",
       "  ' get ice cream with fudge brownies! I LOVE the blonde brownies and cannot get enough of them. This ice cream is by far my favorite. I have driven to multiple stores to try and find it... and when I do',\n",
       "  ' I stock up! I’ve tried other flavors when I can’t find this one',\n",
       "  ' and none of them even come close to making me as happy as this does! I’d say sometimes the caramel to brownie ratio is a little off at times (more brownies please!)',\n",
       "  ' but I’ll still always enjoy it nevertheless!\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Kassidyk',\n",
       "  '2020-07-24',\n",
       "  '1',\n",
       "  'Worst Ice Cream Ever!',\n",
       "  '1.0',\n",
       "  '5.0',\n",
       "  '\"This ice cream is worst ice cream I’ve ever tasted. I was beyond excited when I picked it up today and couldn’t wait to get home and try it. First the sweet cream ice cream has no flavour. But that’s fine I could get past that',\n",
       "  ' but the salted caramel core is what’s really wrong. It’s disgusting and so salty it’s actually inedible. I’ve had ice cream before that I have really enjoyed but never to the point where I physically can not stomach it. I love salted caramel but this is just another level. I actually came here to read the reviews and see how others thought of it just in case mine was a bad batch. Never again!\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Nikiera',\n",
       "  '2020-07-23',\n",
       "  '2',\n",
       "  'Way Too Salty',\n",
       "  '3.0',\n",
       "  '1.0',\n",
       "  '\"I bought this last night to go with Louisiana Crunch cake because I was craving that salty and sweet flavor. Unfortunately',\n",
       "  ' I was not thrilled with the salted caramel core of this ice cream. Usually',\n",
       "  ' when you have salted caramel',\n",
       "  ' the salt enhances the flavor of the caramel. The salt in this core OVERWHELMED everything',\n",
       "  \" even the cake. It was disappointing to not enjoy my dessert and I doubt I'll be finishing this pint of ice cream\",\n",
       "  ' which is a shame because you should never have to throw away ice cream.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Mmelvin',\n",
       "  '2017-05-28',\n",
       "  '3',\n",
       "  '\"Love this flavor',\n",
       "  ' but...\"',\n",
       "  '3.0',\n",
       "  '3.0',\n",
       "  '\"This is definitely my favorite flavor',\n",
       "  ' but recently',\n",
       "  \" it does not have enough blonde brownie chunks. I'll go through at least half of the pint with just 1 or 2 chunks. It looks like plain ice cream with a salted caramel core. This has happened a lot lately\",\n",
       "  \" and it's so dissapointing. I've gotten to the point where I simply stopped buying it. Today\",\n",
       "  ' purchased two because they were on sale. It started the way it should be',\n",
       "  ' with plenty of yummy chunks. Before I even got halfway as I was serving myself',\n",
       "  ' no more chunks. After some digging around',\n",
       "  ' I found 3 more way at the bottom. Please fix this! I love this ice cream',\n",
       "  ' but it\\'s pointless without the chunks.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'Shay10',\n",
       "  '2017-07-02',\n",
       "  '3',\n",
       "  'Really Wanted To Love This',\n",
       "  '4.0',\n",
       "  '1.0',\n",
       "  '\"I am not a chocolate person at all',\n",
       "  \" and this was my very first time trying Ben & Jerry's\",\n",
       "  \" (Shame on me I'm 23 lol) so I was so excited to find a Caramel Ice Cream with BLONDE Brownie chunks instead of fudge. I really wanted to like this ice cream but I just couldn't. The Salted Caramel is a little too salty\",\n",
       "  ' In fact',\n",
       "  \" I couldn't even eat the core\",\n",
       "  \" I just ate around it. I didn't really like the Sweet Cream Ice Cream\",\n",
       "  \" it didn't really have a taste to me\",\n",
       "  ' (Maybe Vanilla would be better). The only thing I loved about this Ice Cream were the Blonde Brownie Chunks',\n",
       "  ' They were so delicious! I would love for this Ice Cream Recipe to be worked on',\n",
       "  ' maybe I\\'ll give it a try again in the future.\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'caramel4dayz',\n",
       "  '2017-07-16',\n",
       "  '2',\n",
       "  'Could be better.',\n",
       "  '8.0',\n",
       "  '6.0',\n",
       "  '\"I LOVE caramel',\n",
       "  ' so much so that I could eat it off a spoon',\n",
       "  ' which was exactly what I was planning to do when I bought this ice cream. Unfortunately this caramel core is way too salty. I actually eat around it! I\\'m not a fan of the sweet cream ice cream flavor either; vanilla would\\'ve worked better. The blondie pieces were delicious but overall I\\'m disappointed. I really wanted to love this since I love Ben & Jerry\\'s but this was not up to par with their other flavors!\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['bj',\n",
       "  '0_bj',\n",
       "  'RosaT777',\n",
       "  '2019-02-12',\n",
       "  '3',\n",
       "  'Salted Caramel core had NO CARAMEL',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  'I love all the Ben & Jerry’s flavor. But recently I been so disappointed of my favorite flavor salted core caramel. It has no core',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '']]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6a819005",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = result_data.map(lambda line: (line[1], int(line[4]), line[6], line[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8a52f98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0_bj', 3, '10.0', '3.0'),\n",
       " ('0_bj', 5, '3.0', '0.0'),\n",
       " ('0_bj', 3, '5.0', '2.0'),\n",
       " ('0_bj', 5, '24.0', '1.0'),\n",
       " ('0_bj', 1, '1.0', '5.0'),\n",
       " ('0_bj', 2, '3.0', '1.0'),\n",
       " ('0_bj', 3, ' but...\"', '3.0'),\n",
       " ('0_bj', 3, '4.0', '1.0'),\n",
       " ('0_bj', 2, '8.0', '6.0'),\n",
       " ('0_bj', 3, '1.0', '1.0')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.take(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79dbe588",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6ae5ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def calculate_weight(helpful_yes, helpful_no):\n",
    "        try:\n",
    "            hy = float(helpful_yes)\n",
    "        except:\n",
    "            return 0\n",
    "        try:\n",
    "            hn = float(helpful_no)\n",
    "        except:\n",
    "            return 0\n",
    "        if (hy + hn) ==0:\n",
    "            return 0\n",
    "        return hy / (hy + hn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8e795b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = result_data.map(lambda line: (line[0], line[1], calculate_weight(line[2], line[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b1843f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0_bj', 3, 0.7692307692307693),\n",
       " ('0_bj', 5, 1.0),\n",
       " ('0_bj', 3, 0.7142857142857143),\n",
       " ('0_bj', 5, 0.96),\n",
       " ('0_bj', 1, 0.16666666666666666),\n",
       " ('0_bj', 2, 0.75),\n",
       " ('0_bj', 3, 0),\n",
       " ('0_bj', 3, 0.8),\n",
       " ('0_bj', 2, 0.5714285714285714),\n",
       " ('0_bj', 3, 0.5)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "413b0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = result_data.map(lambda line: (line[0], (line[1]*line[2], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "36932ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0_bj', (2.307692307692308, 1)),\n",
       " ('0_bj', (5.0, 1)),\n",
       " ('0_bj', (2.142857142857143, 1)),\n",
       " ('0_bj', (4.8, 1)),\n",
       " ('0_bj', (0.16666666666666666, 1)),\n",
       " ('0_bj', (1.5, 1)),\n",
       " ('0_bj', (0, 1)),\n",
       " ('0_bj', (2.4000000000000004, 1)),\n",
       " ('0_bj', (1.1428571428571428, 1)),\n",
       " ('0_bj', (1.5, 1))]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6ea25bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = result_data.reduceByKey(lambda val1, val2: (val1[0] + val2[0], val1[1] + val2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "737990a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0_bj', (94.36007326007326, 208)),\n",
       " ('6_bj', (39.78703703703704, 10)),\n",
       " ('7_bj', (30.0, 7)),\n",
       " ('9_bj', (125.17807017543859, 42)),\n",
       " ('11_bj', (34.08571428571429, 9)),\n",
       " ('13_bj', (101.01336675020886, 133)),\n",
       " ('14_bj', (89.43318903318902, 153)),\n",
       " ('16_bj', (84.21103603603603, 978)),\n",
       " ('18_bj', (84.69215686274511, 111)),\n",
       " ('21_bj', (91.12058175742388, 108))]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "23562d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = result_data.mapValues(lambda x: x[0] / x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ebcbaaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('56_hd', 1.3518518518518519),\n",
       " ('59_hd', 0.8885109666996628),\n",
       " ('60_hd', 1.5065746219592373),\n",
       " ('61_hd', 0.5623482340187146),\n",
       " ('62_hd', 0.6853603603603604),\n",
       " ('63_hd', 1.5138888888888888),\n",
       " ('66_hd', 0.5871290603239602),\n",
       " ('69_hd', 1.9545454545454546),\n",
       " ('0_talenti', 0.7385537457479904),\n",
       " ('2_talenti', 2.097386587771203)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.take(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "76f58a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1094.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/user-data/combined/rating_by_reviews already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-165-a3debbd18038>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mresult_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcoalesce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/user-data/combined/rating_by_reviews'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/pyspark/rdd.py\u001B[0m in \u001B[0;36msaveAsTextFile\u001B[0;34m(self, path, compressionCodecClass)\u001B[0m\n\u001B[1;32m   1654\u001B[0m             \u001B[0mkeyed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBytesToString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcompressionCodec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1655\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1656\u001B[0;31m             \u001B[0mkeyed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBytesToString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaveAsTextFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1657\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1658\u001B[0m     \u001B[0;31m# Pair functions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1305\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1307\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.6/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1094.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/user-data/combined/rating_by_reviews already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "result_data.coalesce(1).saveAsTextFile('/user-data/combined/rating_by_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "31883b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_schema = [\"key_rating\", \"rating_by_reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1281e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "a089f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = spark.createDataFrame(result_data, res_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f9f1c072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key_rating', 'rating_by_reviews']"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c8814be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = spark.read.option(\"header\", \"true\").csv(\"/user-data/combined/products_copy.csv\", quote='\"', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "efbc7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_schema = ['brand', 'key', 'name', 'subhead','description','rating','rating_count','ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "e2d9165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brand',\n",
       " 'key',\n",
       " 'name',\n",
       " 'subhead',\n",
       " 'description',\n",
       " 'rating',\n",
       " 'rating_count',\n",
       " 'ingredients']"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "79529ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------------+--------------------+--------------------+------+------------+--------------------+\n",
      "|brand| key|               name|             subhead|         description|rating|rating_count|         ingredients|\n",
      "+-----+----+-------------------+--------------------+--------------------+------+------------+--------------------+\n",
      "|   bj|0_bj|Salted Caramel Core|Sweet Cream Ice C...|Find your way to ...|   3.7|         208|CREAM, SKIM MILK,...|\n",
      "+-----+----+-------------------+--------------------+--------------------+------+------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prod_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "18d13cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#   you can't modify the existing dataframe as it is immutable, but you can return a new dataframe with the desired modifications.\n",
    "new_df = prod_df.join(res_df, col('key') == col('key_rating'), \"left\"). drop(col('key_rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "462fefb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+--------------------+------+------------+--------------------+------------------+\n",
      "|             brand|                 key|                name|             subhead|         description|rating|rating_count|         ingredients| rating_by_reviews|\n",
      "+------------------+--------------------+--------------------+--------------------+--------------------+------+------------+--------------------+------------------+\n",
      "|Part of our secret| is the quality o...|                null|                null|                null|  null|        null|                null|              null|\n",
      "|Part of our secret| is the quality o...|                null|                null|                null|  null|        null|                null|              null|\n",
      "|                hd|               66_hd|Vanilla Milk Choc...|                null|We start with pur...|   2.5|         224|CREAM, SKIM MILK,...|0.5871290603239602|\n",
      "|           breyers|          17_breyers|     Vanilla Caramel|                null|Torn between swee...|  null|        null|                null|0.1891891891891892|\n",
      "|                bj|               24_bj|Coffee Toffee Bar...|Coffee Ice Cream ...|Coffee What Bar C...|   2.9|         302|CREAM, SKIM MILK,...|1.4508969541838963|\n",
      "+------------------+--------------------+--------------------+--------------------+--------------------+------+------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "580d95cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brand',\n",
       " 'key',\n",
       " 'name',\n",
       " 'subhead',\n",
       " 'description',\n",
       " 'rating',\n",
       " 'rating_count',\n",
       " 'ingredients']"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "70c0a6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------------+--------------------+--------------------+------+------------+--------------------+\n",
      "|brand| key|               name|             subhead|         description|rating|rating_count|         ingredients|\n",
      "+-----+----+-------------------+--------------------+--------------------+------+------------+--------------------+\n",
      "|   bj|0_bj|Salted Caramel Core|Sweet Cream Ice C...|Find your way to ...|   3.7|         208|CREAM, SKIM MILK,...|\n",
      "+-----+----+-------------------+--------------------+--------------------+------+------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prod_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "fda80edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.option(\"header\", \"true\").csv(\"/user-data/combined/products_with_ratings_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e02c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}